\section*{Introduction}

Over the past decade, Neural Networks have been successfully applied to a wide range of tasks in different domains such as computer vision, microbiology, and natural language processing. <Give some examples here>

While the overall structure and semantics of the data being used are different, the overall algorithm and methodology stay the same. This is also true in the field of genomics where neural networks are trained on data that is obtained experimentally using genomic assays like DNase-seq \cite{song2010dnase}, ATAC-seq \cite{buenrostro2015atac}, and ChIP-seq \cite{park2009chip}. Due to the success of these networks in other fields, there has been a shift in which type of architecture to use when looking at this data. For example, sequenced genomic data is represented as a vector of strings, therefore some neural networks treat this as an NLP problem. Others see <some more examples>

This however is an issue as while the data format is similar, the foundation of the data vastly differs. While data collection and understanding of natural language text has a logical process, we are still learning about sequenced genomic data. For example, in genomic assays, the data is highly noisy and possibly mislabeled furthermore a small change or section of the dataset can have a high impact on the overall prediction <Cite?>. Furthermore, if we borrow from computer vision, given an image of a dog, we know which parts are of high importance (i.e ears, tails, and face), and which are not (i.e background, and eye color). This is because we as humans, have an intuitive understanding of dogs, but not genomics. In biology, (cite biochrome), genomic data often include DNA sequences of various widths, which unlike pictures of dogs are not human-readable or verifiable. While there are many issues associated with applying genomic data to machine learning \cite{whalen2022navigating}, machine learning still holds a varying level of success of (words).

This biology-specific issue of nonintuitive data increases the difficulty for deep neural networks, whereas the lack of high-quality datasets contributes to the reproducibility crisis and makes it more difficult to compare architectures, as they are often only evaluated on a custom dataset (see above). This being said, there has been a wide level of success in using neural networks in genomics. A few examples are Bichrome, and DeepCAPE \cite{chen2021deepcape}.


\subsection*{Bichrom}

Bichrom is a bimodal neural network framework that is used for characterizing the relative contribution of both DNA sequences and cell type-specific chomatin. This neural network is broken into two distinct sub-networks Bichrom$_{SEQ}$ and Bichrom$_{CHR}$. Where Bichrom$_{SEQ}$ used one-hot encoded DNA serves as input while the other used binned normalized tag counts from chromatin experiments.

<more information>

\subsection*{DeepCAPE}

DeepCAPE is a deep convolutional neural network that is used to predict enhancers via the integration of DNA sequences and DNAase-seq data. This model has the ability to self adapts to different size datasets and consistently outperforms existing methods in the imbalanced classification of cell line-specific enhancers.

<more information>



