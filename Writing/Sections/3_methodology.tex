\section{Methodology}

\subsection{Data Collection}
\label{Methodology:DC}

\hl{How did we collect the data?}

\subsection{Data processing}

\hl{How did we process the Data}



\subsection{Model Architecture}
\label{Methodology: Model Architecture}

When developing our models we took a modular approach with six different network architectures. For each of our models we have an input label of 500 and and output layer of one with a sigmoid activation function before it. All other activation functions are RELU, have an Adam optimizer for the model with a learning rate of $10^{-4}$ and a Binary Cross Entropy (BCE) loss function. \hl{The average data can be found here}

Our first model was a Deep Neural Network (DNN), for each network that has a DNN section it consists of 8 layers cutting the number of parameters down by roughly half each time. After implementing the DNN we then created a new neural network that implemented a Convolutional Nerual Network with convolutional kernals of 10, 50 and 100 respectivly. \hl{This is because in the preprocessing stage we had window sizes of similar values}. After these convolutions we flattened the output and read it into a Deep Neural Network. This CNN was used to allow us to extract spatial information from the data that can be used for better prediction of enhancers. 

After implementing the CNN, we moved towards an Long Short Term Memory model with an attached DNN. The LSTM section was 5 layers deep with an output of 30 neuron. The output of this was then fed into a DNN that was the same as the inital DNN. We then combined the CNN and LSTM layers with a DNN in the following model to extract both a temporal and spatial representation of the data. The last two models we focused on were the DNN to LSTM and CNN to LSTM for similar reasons. 