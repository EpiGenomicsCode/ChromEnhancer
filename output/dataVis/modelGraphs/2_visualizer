digraph {
	graph [size="24.75,24.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140165610346928 [label="
 (500, 1)" fillcolor=darkolivegreen1]
	140165610452176 [label=SigmoidBackward0]
	140165610453520 -> 140165610452176
	140165610453520 [label=AddmmBackward0]
	140165610452688 -> 140165610453520
	140165610464752 [label="DNN.layer_out.bias
 (1)" fillcolor=lightblue]
	140165610464752 -> 140165610452688
	140165610452688 [label=AccumulateGrad]
	140165610452432 -> 140165610453520
	140165610452432 [label=ReluBackward0]
	140165610451792 -> 140165610452432
	140165610451792 [label=AddmmBackward0]
	140165610452112 -> 140165610451792
	140165610464560 [label="DNN.layer_7.bias
 (3)" fillcolor=lightblue]
	140165610464560 -> 140165610452112
	140165610452112 [label=AccumulateGrad]
	140165610451984 -> 140165610451792
	140165610451984 [label=ReluBackward0]
	140165610452304 -> 140165610451984
	140165610452304 [label=AddmmBackward0]
	140165610451856 -> 140165610452304
	140165610464368 [label="DNN.layer_6.bias
 (7)" fillcolor=lightblue]
	140165610464368 -> 140165610451856
	140165610451856 [label=AccumulateGrad]
	140165610453712 -> 140165610452304
	140165610453712 [label=ReluBackward0]
	140165610452880 -> 140165610453712
	140165610452880 [label=AddmmBackward0]
	140165610451600 -> 140165610452880
	140165610464176 [label="DNN.layer_5.bias
 (15)" fillcolor=lightblue]
	140165610464176 -> 140165610451600
	140165610451600 [label=AccumulateGrad]
	140165610451408 -> 140165610452880
	140165610451408 [label=ReluBackward0]
	140165610453584 -> 140165610451408
	140165610453584 [label=AddmmBackward0]
	140165610451344 -> 140165610453584
	140165610463984 [label="DNN.layer_4.bias
 (31)" fillcolor=lightblue]
	140165610463984 -> 140165610451344
	140165610451344 [label=AccumulateGrad]
	140165610451152 -> 140165610453584
	140165610451152 [label=ReluBackward0]
	140165610451280 -> 140165610451152
	140165610451280 [label=AddmmBackward0]
	140165610453136 -> 140165610451280
	140165610463792 [label="DNN.layer_3.bias
 (62)" fillcolor=lightblue]
	140165610463792 -> 140165610453136
	140165610453136 [label=AccumulateGrad]
	140165610451088 -> 140165610451280
	140165610451088 [label=ReluBackward0]
	140165610451024 -> 140165610451088
	140165610451024 [label=AddmmBackward0]
	140165610452944 -> 140165610451024
	140165610463600 [label="DNN.layer_2.bias
 (125)" fillcolor=lightblue]
	140165610463600 -> 140165610452944
	140165610452944 [label=AccumulateGrad]
	140165610454032 -> 140165610451024
	140165610454032 [label=ReluBackward0]
	140165610453968 -> 140165610454032
	140165610453968 [label=AddmmBackward0]
	140165610454480 -> 140165610453968
	140165610463312 [label="DNN.layer_1.bias
 (250)" fillcolor=lightblue]
	140165610463312 -> 140165610454480
	140165610454480 [label=AccumulateGrad]
	140165610453648 -> 140165610453968
	140165610453648 [label=ReluBackward0]
	140165610454736 -> 140165610453648
	140165610454736 [label=AddmmBackward0]
	140165610453776 -> 140165610454736
	140165610506672 [label="layer_4.bias
 (500)" fillcolor=lightblue]
	140165610506672 -> 140165610453776
	140165610453776 [label=AccumulateGrad]
	140165610454352 -> 140165610454736
	140165610454352 [label=ReshapeAliasBackward0]
	140165610451472 -> 140165610454352
	140165610451472 [label=ReluBackward0]
	140165610454608 -> 140165610451472
	140165610454608 [label=ConvolutionBackward0]
	140165610454544 -> 140165610454608
	140165610454544 [label=ReluBackward0]
	140165532561616 -> 140165610454544
	140165532561616 [label=ConvolutionBackward0]
	140165532561808 -> 140165532561616
	140165532561808 [label=ReluBackward0]
	140165532562000 -> 140165532561808
	140165532562000 [label=ConvolutionBackward0]
	140165532562192 -> 140165532562000
	140165610506864 [label="layer_1.weight
 (3, 1, 10)" fillcolor=lightblue]
	140165610506864 -> 140165532562192
	140165532562192 [label=AccumulateGrad]
	140165532562320 -> 140165532562000
	140165610506768 [label="layer_1.bias
 (3)" fillcolor=lightblue]
	140165610506768 -> 140165532562320
	140165532562320 [label=AccumulateGrad]
	140165532561936 -> 140165532561616
	140165610506960 [label="layer_2.weight
 (5, 3, 50)" fillcolor=lightblue]
	140165610506960 -> 140165532561936
	140165532561936 [label=AccumulateGrad]
	140165532561680 -> 140165532561616
	140165610507056 [label="layer_2.bias
 (5)" fillcolor=lightblue]
	140165610507056 -> 140165532561680
	140165532561680 [label=AccumulateGrad]
	140165610454928 -> 140165610454608
	140165610507152 [label="layer_3.weight
 (10, 5, 100)" fillcolor=lightblue]
	140165610507152 -> 140165610454928
	140165610454928 [label=AccumulateGrad]
	140165532561552 -> 140165610454608
	140165610507248 [label="layer_3.bias
 (10)" fillcolor=lightblue]
	140165610507248 -> 140165532561552
	140165532561552 [label=AccumulateGrad]
	140165610454288 -> 140165610454736
	140165610454288 [label=TBackward0]
	140165610454672 -> 140165610454288
	140165610507440 [label="layer_4.weight
 (500, 3430)" fillcolor=lightblue]
	140165610507440 -> 140165610454672
	140165610454672 [label=AccumulateGrad]
	140165610454864 -> 140165610453968
	140165610454864 [label=TBackward0]
	140165610453328 -> 140165610454864
	140165610463408 [label="DNN.layer_1.weight
 (250, 500)" fillcolor=lightblue]
	140165610463408 -> 140165610453328
	140165610453328 [label=AccumulateGrad]
	140165610454160 -> 140165610451024
	140165610454160 [label=TBackward0]
	140165610453904 -> 140165610454160
	140165610463504 [label="DNN.layer_2.weight
 (125, 250)" fillcolor=lightblue]
	140165610463504 -> 140165610453904
	140165610453904 [label=AccumulateGrad]
	140165610453008 -> 140165610451280
	140165610453008 [label=TBackward0]
	140165610453200 -> 140165610453008
	140165610463696 [label="DNN.layer_3.weight
 (62, 125)" fillcolor=lightblue]
	140165610463696 -> 140165610453200
	140165610453200 [label=AccumulateGrad]
	140165610453264 -> 140165610453584
	140165610453264 [label=TBackward0]
	140165610453392 -> 140165610453264
	140165610463888 [label="DNN.layer_4.weight
 (31, 62)" fillcolor=lightblue]
	140165610463888 -> 140165610453392
	140165610453392 [label=AccumulateGrad]
	140165610453456 -> 140165610452880
	140165610453456 [label=TBackward0]
	140165610451664 -> 140165610453456
	140165610464080 [label="DNN.layer_5.weight
 (15, 31)" fillcolor=lightblue]
	140165610464080 -> 140165610451664
	140165610451664 [label=AccumulateGrad]
	140165610451728 -> 140165610452304
	140165610451728 [label=TBackward0]
	140165610451920 -> 140165610451728
	140165610464272 [label="DNN.layer_6.weight
 (7, 15)" fillcolor=lightblue]
	140165610464272 -> 140165610451920
	140165610451920 [label=AccumulateGrad]
	140165610454096 -> 140165610451792
	140165610454096 [label=TBackward0]
	140165610452368 -> 140165610454096
	140165610464464 [label="DNN.layer_7.weight
 (3, 7)" fillcolor=lightblue]
	140165610464464 -> 140165610452368
	140165610452368 [label=AccumulateGrad]
	140165610452496 -> 140165610453520
	140165610452496 [label=TBackward0]
	140165610452240 -> 140165610452496
	140165610464656 [label="DNN.layer_out.weight
 (1, 3)" fillcolor=lightblue]
	140165610464656 -> 140165610452240
	140165610452240 [label=AccumulateGrad]
	140165610452176 -> 140165610346928
}
